{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install once\n",
    "!pip install -U boto3 sagemaker awscli\n",
    "# restart jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3, os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sm-qwen2-multinode-2024-11-06-01-05-48-380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-06 01:05:52 Starting - Starting the training job\n",
      "2024-11-06 01:05:52 Pending - Training job waiting for capacity......\n",
      "2024-11-06 01:06:42 Pending - Preparing the instances for training...\n",
      "2024-11-06 01:07:24 Downloading - Downloading the training image.....................\n",
      "2024-11-06 01:10:47 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:14,799 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:14,816 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:14,826 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:14,828 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:16,740 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.14.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 41.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (5.9.8)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.7.1)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2024.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400400 sha256=712610e20f71b9bf0f539761128f352e2fa17a9c4eab12f5c334bc63fa8f4244\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/21/93/10/aca4f9f9390297a80a58fb8db0fcdcf1f41499d1afe922a513\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, hjson, pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 hjson-3.1.0 py-cpuinfo-9.0.0 pynvml-11.5.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.2 -> 24.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,739 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,739 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,780 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,808 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,836 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,846 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"sm-qwen2-multinode-2024-11-06-01-05-48-380\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-11-06-01-05-48-380/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"estimator_entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"estimator_entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=estimator_entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=estimator_entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-11-06-01-05-48-380/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"sm-qwen2-multinode-2024-11-06-01-05-48-380\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-11-06-01-05-48-380/source/sourcedir.tar.gz\",\"module_name\":\"estimator_entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"estimator_entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.11 estimator_entry.py\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,847 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:11:24,847 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/LLaMA-Factory\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers>=4.41.2 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.16.0 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (0.30.1)\u001b[0m\n",
      "\u001b[34mCollecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (2.7.1)\u001b[0m\n",
      "\u001b[34mCollecting fastapi (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting sse-starlette (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting fire (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading fire-0.7.0.tar.gz (87 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.11/site-packages (from llamafactory==0.8.3.dev0) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (2.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (16.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0) (2024.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==1.4.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.11/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.11/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.5)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.11/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting pydub (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart==0.0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting safehttpx<1.0,>=0.1.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting starlette<1.0,>=0.40.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.11/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.11.0)\u001b[0m\n",
      "\u001b[34mCollecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.52.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic->llamafactory==0.8.3.dev0) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic->llamafactory==0.8.3.dev0) (2.18.2)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.21,>=0.20 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.30.1 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from trl>=0.8.6->llamafactory==0.8.3.dev0) (13.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting termcolor (from fire->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.7)\u001b[0m\n",
      "\u001b[34mCollecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.12.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.8.30)\u001b[0m\n",
      "\u001b[34mCollecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.26.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl>=0.8.6->llamafactory==0.8.3.dev0) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl>=0.8.6->llamafactory==0.8.3.dev0) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl>=0.8.6->llamafactory==0.8.3.dev0) (0.1.2)\u001b[0m\n",
      "\u001b[34mCollecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\u001b[0m\n",
      "\u001b[34mDownloading gradio-5.5.0-py3-none-any.whl (56.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.7/56.7 MB 128.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.115.4-py3-none-any.whl (94 kB)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.13.2-py3-none-any.whl (320 kB)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 61.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.12.0-py3-none-any.whl (310 kB)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.1.0-py3-none-any.whl (333 kB)\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 120.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 110.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.8/792.8 kB 81.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/11.0 MB 165.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.41.2-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 124.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading typer-0.12.5-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: llamafactory, fire\u001b[0m\n",
      "\u001b[34mBuilding editable for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding editable for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=18861 sha256=dcc666c89a60f81386071a0e9fae2ee83ab2ed03dbb5b8b3f8c3be114dc5fee1\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-6t5im0i7/wheels/6b/38/9f/e1135c70550b8bab6bd9ee2960b7c0a28812185b0b02b9a215\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=af778013df01ab41b50b32cbcec5f0cc7f71fe4f1f649e37cc470281a5b5e593\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\u001b[0m\n",
      "\u001b[34mSuccessfully built llamafactory fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, pydub, xxhash, websockets, tomlkit, termcolor, sniffio, semantic-version, ruff, regex, python-multipart, propcache, orjson, multidict, h11, frozenlist, ffmpy, aiohappyeyeballs, aiofiles, yarl, uvicorn, tiktoken, huggingface-hub, httpcore, fire, anyio, aiosignal, typer, tokenizers, starlette, httpx, aiohttp, accelerate, transformers, sse-starlette, safehttpx, gradio-client, fastapi, peft, gradio, datasets, trl, llamafactory\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface_hub 0.23.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface_hub-0.23.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface_hub-0.23.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typer\u001b[0m\n",
      "\u001b[34mFound existing installation: typer 0.9.4\u001b[0m\n",
      "\u001b[34mUninstalling typer-0.9.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typer-0.9.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.30.1\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.30.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.30.1\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.5 which is incompatible.\u001b[0m\n",
      "\u001b[34mweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.5 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-1.1.0 aiofiles-23.2.1 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 anyio-4.6.2.post1 datasets-3.1.0 fastapi-0.115.4 ffmpy-0.4.0 fire-0.7.0 frozenlist-1.5.0 gradio-5.5.0 gradio-client-1.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.2 llamafactory-0.8.3.dev0 multidict-6.1.0 orjson-3.10.11 peft-0.13.2 propcache-0.2.0 pydub-0.25.1 python-multipart-0.0.12 regex-2024.9.11 ruff-0.7.2 safehttpx-0.1.1 semantic-version-2.10.0 sentencepiece-0.2.0 sniffio-1.3.1 sse-starlette-2.1.3 starlette-0.41.2 termcolor-2.5.0 tiktoken-0.8.0 tokenizers-0.20.3 tomlkit-0.12.0 transformers-4.46.2 trl-0.12.0 typer-0.12.5 uvicorn-0.32.0 websockets-12.0 xxhash-3.5.0 yarl-1.17.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.2 -> 24.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34msh: 1: wandb: not found\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/.gitattributes.metadata /tmp/initial-model-path/.cache/huggingface/download/.gitattributes.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/.gitattributes.lock /tmp/initial-model-path/.cache/huggingface/download/.gitattributes.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/config.json.lock /tmp/initial-model-path/.cache/huggingface/download/config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/LICENSE.lock /tmp/initial-model-path/.cache/huggingface/download/LICENSE.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/LICENSE.metadata /tmp/initial-model-path/.cache/huggingface/download/LICENSE.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/README.md.lock /tmp/initial-model-path/.cache/huggingface/download/README.md.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/.gitignore /tmp/initial-model-path/.cache/huggingface/.gitignore\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/generation_config.json.lock /tmp/initial-model-path/.cache/huggingface/download/generation_config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/README.md.metadata /tmp/initial-model-path/.cache/huggingface/download/README.md.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/generation_config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/generation_config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/merges.txt.lock /tmp/initial-model-path/.cache/huggingface/download/merges.txt.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/model.safetensors.lock /tmp/initial-model-path/.cache/huggingface/download/model.safetensors.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/model.safetensors.metadata /tmp/initial-model-path/.cache/huggingface/download/model.safetensors.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/vocab.json.lock /tmp/initial-model-path/.cache/huggingface/download/vocab.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/tokenizer.json.lock /tmp/initial-model-path/.cache/huggingface/download/tokenizer.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/vocab.json.metadata /tmp/initial-model-path/.cache/huggingface/download/vocab.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/tokenizer_config.json.lock /tmp/initial-model-path/.cache/huggingface/download/tokenizer_config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/.gitignore /tmp/initial-model-path/.huggingface/.gitignore\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/tokenizer.json.metadata /tmp/initial-model-path/.cache/huggingface/download/tokenizer.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/tokenizer_config.json.metadata /tmp/initial-model-path/.cache/huggingface/download/tokenizer_config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.cache/huggingface/download/merges.txt.metadata /tmp/initial-model-path/.cache/huggingface/download/merges.txt.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.gitattributes /tmp/initial-model-path/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/.gitattributes.lock /tmp/initial-model-path/.huggingface/download/.gitattributes.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/.gitattributes.metadata /tmp/initial-model-path/.huggingface/download/.gitattributes.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/README.md.metadata /tmp/initial-model-path/.huggingface/download/README.md.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/config.json.metadata /tmp/initial-model-path/.huggingface/download/config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/LICENSE.metadata /tmp/initial-model-path/.huggingface/download/LICENSE.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/README.md.lock /tmp/initial-model-path/.huggingface/download/README.md.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/merges.txt.lock /tmp/initial-model-path/.huggingface/download/merges.txt.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/merges.txt.metadata /tmp/initial-model-path/.huggingface/download/merges.txt.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/config.json.lock /tmp/initial-model-path/.huggingface/download/config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/LICENSE.lock /tmp/initial-model-path/.huggingface/download/LICENSE.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/model.safetensors.metadata /tmp/initial-model-path/.huggingface/download/model.safetensors.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/model.safetensors.lock /tmp/initial-model-path/.huggingface/download/model.safetensors.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/generation_config.json.lock /tmp/initial-model-path/.huggingface/download/generation_config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/generation_config.json.metadata /tmp/initial-model-path/.huggingface/download/generation_config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer.json.metadata /tmp/initial-model-path/.huggingface/download/tokenizer.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer.json.lock /tmp/initial-model-path/.huggingface/download/tokenizer.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer_config.json.metadata /tmp/initial-model-path/.huggingface/download/tokenizer_config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer_config.json.lock /tmp/initial-model-path/.huggingface/download/tokenizer_config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/vocab.json.metadata /tmp/initial-model-path/.huggingface/download/vocab.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/vocab.json.lock /tmp/initial-model-path/.huggingface/download/vocab.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/LICENSE /tmp/initial-model-path/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/generation_config.json /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/README.md /tmp/initial-model-path/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/tokenizer_config.json /tmp/initial-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/config.json /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/merges.txt /tmp/initial-model-path/merges.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/vocab.json /tmp/initial-model-path/vocab.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/tokenizer.json /tmp/initial-model-path/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/model.safetensors /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/qwen2-train-dataset/alpaca_zh_demo.json /tmp/data-path/alpaca_zh_demo.json\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:11:57,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:11:59,745] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:11:59,745] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m11/06/2024 01:11:59 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2209] 2024-11-06 01:11:59,796 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2475] 2024-11-06 01:12:00,096 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2475] 2024-11-06 01:12:00,096 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:00 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:00 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 60061.06 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 7708.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):  25%|██▌       | 250/1000 [00:00<00:02, 322.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 640.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:01<00:00, 883.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4): 100%|██████████| 1000/1000 [00:01<00:00, 1153.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4): 100%|██████████| 1000/1000 [00:01<00:00, 806.92 examples/s]\u001b[0m\n",
      "\u001b[34minput_ids:\u001b[0m\n",
      "\u001b[34m[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 102450, 62926, 104136, 89012, 22382, 44177, 101047, 100369, 99891, 101911, 5122, 102150, 101911, 33108, 8903, 63109, 36587, 1773, 151645, 198, 151644, 77091, 198, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\u001b[0m\n",
      "\u001b[34minputs:\u001b[0m\n",
      "\u001b[34m<|im_start|>system\u001b[0m\n",
      "\u001b[34mYou are a helpful assistant.<|im_end|>\u001b[0m\n",
      "\u001b[34m<|im_start|>user\u001b[0m\n",
      "\u001b[34m识别并解释给定列表中的两个科学理论：细胞理论和日心说。<|im_end|>\u001b[0m\n",
      "\u001b[34m<|im_start|>assistant\u001b[0m\n",
      "\u001b[34m细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\u001b[0m\n",
      "\u001b[34m日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\u001b[0m\n",
      "\u001b[34mlabel_ids:\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\u001b[0m\n",
      "\u001b[34mlabels:\u001b[0m\n",
      "\u001b[34m细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\u001b[0m\n",
      "\u001b[34m日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:677] 2024-11-06 01:12:01,788 >> loading configuration file /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:677] 2024-11-06 01:12:01,788 >> loading configuration file /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:746] 2024-11-06 01:12:01,789 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/initial-model-path\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:746] 2024-11-06 01:12:01,789 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/initial-model-path\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3934] 2024-11-06 01:12:01,819 >> loading weights file /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3934] 2024-11-06 01:12:01,819 >> loading weights file /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4080] 2024-11-06 01:12:01,827 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4080] 2024-11-06 01:12:01,827 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1096] 2024-11-06 01:12:01,837 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1096] 2024-11-06 01:12:01,837 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mNCCL version 2.21.5+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:211:296 [0] nccl_net_ofi_create_plugin:204 NCCL WARN NET/OFI Failed to initialize sendrecv protocol\u001b[0m\n",
      "\u001b[34malgo-1:211:296 [0] nccl_net_ofi_create_plugin:257 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:04,238] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.63B\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4800] 2024-11-06 01:12:04,708 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4808] 2024-11-06 01:12:04,708 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /tmp/initial-model-path.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4800] 2024-11-06 01:12:04,708 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4808] 2024-11-06 01:12:04,708 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /tmp/initial-model-path.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1049] 2024-11-06 01:12:04,710 >> loading configuration file /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1049] 2024-11-06 01:12:04,710 >> loading configuration file /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1096] 2024-11-06 01:12:04,711 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1096] 2024-11-06 01:12:04,711 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:04 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:04 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:04 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:04 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[34m11/06/2024 01:12:04 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[34m/opt/ml/code/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\u001b[0m\n",
      "\u001b[34m[WARNING|trainer.py:649] 2024-11-06 01:12:04,765 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[WARNING|trainer.py:649] 2024-11-06 01:12:04,765 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:699] 2024-11-06 01:12:04,765 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:699] 2024-11-06 01:12:04,765 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:334] 2024-11-06 01:12:04,969 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:334] 2024-11-06 01:12:04,969 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py311_cu121/cpu_adam...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py311_cu121/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \u001b[0m\n",
      "\u001b[34mIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc --generate-dependencies-with-compile --dependency-output custom_cuda_kernel.cuda.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 33.80791234970093 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000002, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,964] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,965] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,965] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,977] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,977] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,977] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:40,977] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,170] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,171] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.76 GB         CA 0.0 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,171] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.41 GB, percent = 20.7%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,172] [INFO] [stage3.py:130:__init__] Reduce bucket size 802816\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,172] [INFO] [stage3.py:131:__init__] Prefetch bucket size 722534\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,363] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,364] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,364] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.41 GB, percent = 20.7%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 71552 in 121 params\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,574] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,575] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,575] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.41 GB, percent = 20.7%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,769] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,770] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:41,770] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.41 GB, percent = 20.7%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:42,658] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:42,659] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:42,659] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.43 GB, percent = 24.0%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:42,859] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:42,859] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:42,860] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.43 GB, percent = 24.0%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:43,470] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:43,471] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:43,471] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 9.27 GB, percent = 29.9%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:43,669] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:43,669] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:43,669] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 9.27 GB, percent = 29.9%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:46,112] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:46,113] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:46,113] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.31 GB, percent = 36.5%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:46,114] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,278] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,279] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.51 GB         CA 0.53 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,279] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.33 GB, percent = 39.8%\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,279] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,279] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,279] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-06, 2e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,280] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": true, \n",
      "    \"contiguous_memory_optimization\": true, \n",
      "    \"cpu_checkpointing\": true, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9d603bf610>\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,281] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   graph_harvesting ............. False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,282] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   train_batch_size ............. 1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   world_size ................... 1\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=802816 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=722534 param_persistence_threshold=8960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:12:47,283] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true, \n",
      "        \"cpu_checkpointing\": true, \n",
      "        \"contiguous_memory_optimization\": true, \n",
      "        \"synchronize_checkpoint_boundary\": false, \n",
      "        \"profile\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 8.028160e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 7.225340e+05, \n",
      "        \"stage3_param_persistence_threshold\": 8.960000e+03, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2314] 2024-11-06 01:12:47,285 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2314] 2024-11-06 01:12:47,285 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2315] 2024-11-06 01:12:47,285 >>   Num examples = 1,000\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2316] 2024-11-06 01:12:47,285 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2315] 2024-11-06 01:12:47,285 >>   Num examples = 1,000\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2316] 2024-11-06 01:12:47,285 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2317] 2024-11-06 01:12:47,285 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2320] 2024-11-06 01:12:47,285 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2317] 2024-11-06 01:12:47,285 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2320] 2024-11-06 01:12:47,285 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2321] 2024-11-06 01:12:47,285 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2322] 2024-11-06 01:12:47,285 >>   Total optimization steps = 10\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2321] 2024-11-06 01:12:47,285 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2322] 2024-11-06 01:12:47,285 >>   Total optimization steps = 10\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2323] 2024-11-06 01:12:47,286 >>   Number of trainable parameters = 494,032,768\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2323] 2024-11-06 01:12:47,286 >>   Number of trainable parameters = 494,032,768\u001b[0m\n",
      "\u001b[34m0%|          | 0/10 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 1/10 [00:02<00:25,  2.80s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 2/10 [00:06<00:28,  3.52s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 3/10 [00:08<00:18,  2.63s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 4/10 [00:09<00:13,  2.21s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 5/10 [00:11<00:09,  1.97s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 6/10 [00:13<00:07,  1.85s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 7/10 [00:14<00:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 8/10 [00:16<00:03,  1.71s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 9/10 [00:17<00:01,  1.66s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [00:19<00:00,  1.61s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3812] 2024-11-06 01:13:07,431 >> Saving model checkpoint to /tmp/tuned-model-path/checkpoint-10\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3812] 2024-11-06 01:13:07,431 >> Saving model checkpoint to /tmp/tuned-model-path/checkpoint-10\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:414] 2024-11-06 01:13:07,434 >> Configuration saved in /tmp/tuned-model-path/checkpoint-10/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:414] 2024-11-06 01:13:07,434 >> Configuration saved in /tmp/tuned-model-path/checkpoint-10/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:865] 2024-11-06 01:13:07,434 >> Configuration saved in /tmp/tuned-model-path/checkpoint-10/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:865] 2024-11-06 01:13:07,434 >> Configuration saved in /tmp/tuned-model-path/checkpoint-10/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3035] 2024-11-06 01:13:08,987 >> Model weights saved in /tmp/tuned-model-path/checkpoint-10/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3035] 2024-11-06 01:13:08,987 >> Model weights saved in /tmp/tuned-model-path/checkpoint-10/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2646] 2024-11-06 01:13:08,988 >> tokenizer config file saved in /tmp/tuned-model-path/checkpoint-10/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2646] 2024-11-06 01:13:08,988 >> tokenizer config file saved in /tmp/tuned-model-path/checkpoint-10/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2655] 2024-11-06 01:13:08,988 >> Special tokens file saved in /tmp/tuned-model-path/checkpoint-10/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2655] 2024-11-06 01:13:08,988 >> Special tokens file saved in /tmp/tuned-model-path/checkpoint-10/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:09,222] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is about to be saved!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:09,229] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /tmp/tuned-model-path/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:09,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tuned-model-path/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:09,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tuned-model-path/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:09,242] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tuned-model-path/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:35,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tuned-model-path/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:35,882] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /tmp/tuned-model-path/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-11-06 01:13:35,885] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10 is ready now!\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2591] 2024-11-06 01:13:35,887 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2591] 2024-11-06 01:13:35,887 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 48.6005, 'train_samples_per_second': 0.206, 'train_steps_per_second': 0.206, 'train_loss': 1.8162588119506835, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [00:48<00:00,  1.61s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [00:48<00:00,  4.86s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3812] 2024-11-06 01:13:36,409 >> Saving model checkpoint to /tmp/tuned-model-path\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3812] 2024-11-06 01:13:36,409 >> Saving model checkpoint to /tmp/tuned-model-path\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:414] 2024-11-06 01:13:36,411 >> Configuration saved in /tmp/tuned-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:414] 2024-11-06 01:13:36,411 >> Configuration saved in /tmp/tuned-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:865] 2024-11-06 01:13:36,412 >> Configuration saved in /tmp/tuned-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:865] 2024-11-06 01:13:36,412 >> Configuration saved in /tmp/tuned-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3035] 2024-11-06 01:13:43,212 >> Model weights saved in /tmp/tuned-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3035] 2024-11-06 01:13:43,212 >> Model weights saved in /tmp/tuned-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2646] 2024-11-06 01:13:43,213 >> tokenizer config file saved in /tmp/tuned-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2646] 2024-11-06 01:13:43,213 >> tokenizer config file saved in /tmp/tuned-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2655] 2024-11-06 01:13:43,213 >> Special tokens file saved in /tmp/tuned-model-path/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2655] 2024-11-06 01:13:43,213 >> Special tokens file saved in /tmp/tuned-model-path/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       0.01\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     1.8163\n",
      "  train_runtime            = 0:00:48.60\n",
      "  train_samples_per_second =      0.206\n",
      "  train_steps_per_second   =      0.206\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2024-11-06 01:13:43,433 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2024-11-06 01:13:43,433 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/added_tokens.json s3://sagemaker-us-east-1-633205212955/output-model/2408/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/latest s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/latest\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/trainer_state.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/trainer_state.json s3://sagemaker-us-east-1-633205212955/output-model/2408/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/added_tokens.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/tokenizer_config.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/generation_config.json s3://sagemaker-us-east-1-633205212955/output-model/2408/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/README.md s3://sagemaker-us-east-1-633205212955/output-model/2408/README.md\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/generation_config.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/all_results.json s3://sagemaker-us-east-1-633205212955/output-model/2408/all_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/scheduler.pt s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/scheduler.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/config.json s3://sagemaker-us-east-1-633205212955/output-model/2408/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/trainer_log.jsonl s3://sagemaker-us-east-1-633205212955/output-model/2408/trainer_log.jsonl\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/tokenizer_config.json s3://sagemaker-us-east-1-633205212955/output-model/2408/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/training_args.bin s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/training_args.bin s3://sagemaker-us-east-1-633205212955/output-model/2408/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/rng_state.pth s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/rng_state.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/special_tokens_map.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/train_results.json s3://sagemaker-us-east-1-633205212955/output-model/2408/train_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/config.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/special_tokens_map.json s3://sagemaker-us-east-1-633205212955/output-model/2408/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/zero_to_fp32.py s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/zero_to_fp32.py\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/vocab.json s3://sagemaker-us-east-1-633205212955/output-model/2408/vocab.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/vocab.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/vocab.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/merges.txt s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/merges.txt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/merges.txt s3://sagemaker-us-east-1-633205212955/output-model/2408/merges.txt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/tokenizer.json s3://sagemaker-us-east-1-633205212955/output-model/2408/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/tokenizer.json s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/model.safetensors s3://sagemaker-us-east-1-633205212955/output-model/2408/model.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/model.safetensors s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/model.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m2024-11-06 01:14:11,182 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:14:11,182 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-06 01:14:11,183 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-06 01:14:28 Uploading - Uploading generated training model\n",
      "2024-11-06 01:14:28 Completed - Resource retained for reuse\n",
      "Training seconds: 431\n",
      "Billable seconds: 431\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "# image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker'\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker'\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"    # 1 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.g5.12xlarge\"     # 4 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.g5.48xlarge\"    # 8 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.p4d.24xlarge\"   # 8 * A100 (40G/GPU)\n",
    "# instance_type = \"ml.p5.48xlarge\"    # 8 * H100 (80G/GPU)\n",
    "\n",
    "instance_count = 1                  # 1 or Multi-node\n",
    "\n",
    "envs = {\n",
    "    \"DATA_S3_PATH\": f's3://{sagemaker_default_bucket}/qwen2-train-dataset/*',\n",
    "    'MODEL_ID_OR_S3_PATH': f's3://{sagemaker_default_bucket}/Qwen2-0.5B-Instruct/*',\n",
    "    'MODEL_SAVE_PATH_S3': f's3://{sagemaker_default_bucket}/output-model/2408/'\n",
    "}\n",
    "\n",
    "hypers = {\n",
    "}\n",
    "\n",
    "smp_estimator = Estimator(role=role,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name='sm-qwen2-multinode',\n",
    "    entry_point=\"estimator_entry.py\",\n",
    "    source_dir='submit_src/',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    environment=envs,\n",
    "    hyperparameters=hypers,\n",
    "    image_uri=image_uri,\n",
    "    max_run=7200,\n",
    "    keep_alive_period_in_seconds=60,\n",
    "    enable_remote_debug=True,\n",
    "    disable_output_compression=True,\n",
    ")\n",
    "\n",
    "smp_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-633205212955/output-model/2408/\n"
     ]
    }
   ],
   "source": [
    "!echo s3://$sagemaker_default_bucket/output-model/2408/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE checkpoint-10/\n",
      "2024-11-06 01:13:49       1236 README.md\n",
      "2024-11-06 01:13:49         80 added_tokens.json\n",
      "2024-11-06 01:13:49        196 all_results.json\n",
      "2024-11-06 01:13:49        729 config.json\n",
      "2024-11-06 01:13:49        242 generation_config.json\n",
      "2024-11-06 01:13:49    1671853 merges.txt\n",
      "2024-11-06 01:13:49  988097824 model.safetensors\n",
      "2024-11-06 01:13:49        367 special_tokens_map.json\n",
      "2024-11-06 01:13:49   11418266 tokenizer.json\n",
      "2024-11-06 01:13:49       1533 tokenizer_config.json\n",
      "2024-11-06 01:13:49        196 train_results.json\n",
      "2024-11-06 01:13:49        174 trainer_log.jsonl\n",
      "2024-11-06 01:13:49        969 trainer_state.json\n",
      "2024-11-06 01:13:49       7608 training_args.bin\n",
      "2024-11-06 01:13:49    2776833 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://$sagemaker_default_bucket/output-model/2408/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/added_tokens.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/special_tokens_map.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/config.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/latest\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/generation_config.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/tokenizer.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/rng_state.pth\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/scheduler.pt\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/tokenizer_config.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/trainer_state.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/vocab.json\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/training_args.bin\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/zero_to_fp32.py\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/model.safetensors\n",
      "delete: s3://sagemaker-us-east-1-633205212955/output-model/2408/checkpoint-10/merges.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://$sagemaker_default_bucket/output-model/2408/checkpoint-10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
