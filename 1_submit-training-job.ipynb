{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install once\n",
    "!pip install -U boto3 sagemaker awscli\n",
    "# restart jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3, os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sm-qwen2-multinode-2024-07-11-00-25-58-687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 00:26:02 Starting - Starting the training job...\n",
      "2024-07-11 00:26:02 Pending - Training job waiting for capacity......\n",
      "2024-07-11 00:27:20 Pending - Preparing the instances for training...\n",
      "2024-07-11 00:27:59 Downloading - Downloading input data...\n",
      "2024-07-11 00:28:09 Downloading - Downloading the training image.....................\n",
      "2024-07-11 00:31:42 Training - Training image download completed. Training in progress..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:07,217 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:07,252 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:07,263 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:07,265 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:07,248 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:07,284 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:07,296 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:07,297 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:08,990 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:09,003 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting deepspeed==0.14.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 42.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.14.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 38.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (5.9.8)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.18.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2024.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.11.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (5.9.8)\u001b[0m\n",
      "\u001b[35mCollecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.7.2)\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.0->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.18.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (3.1.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2024.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[35mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 9.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 9.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.14.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 8.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400398 sha256=e9c7259a56e4644f8afcb839de20bd59de3dd9652ae8d4fe80efe6ce808a185d\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[35mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400400 sha256=3ec021555bf5213eef3391631a33604d69dd25756c247c5e3aca81b3984cd649\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[35mInstalling collected packages: py-cpuinfo, hjson, pynvml, deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, hjson, pynvml, deepspeed\u001b[0m\n",
      "\u001b[35mSuccessfully installed deepspeed-0.14.0 hjson-3.1.0 py-cpuinfo-9.0.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 24.0 -> 24.1.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,430 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,430 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,485 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,532 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,579 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,591 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"sm-qwen2-multinode-2024-07-11-00-25-58-687\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-07-11-00-25-58-687/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"estimator_entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"estimator_entry.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=estimator_entry.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=estimator_entry\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-07-11-00-25-58-687/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"sm-qwen2-multinode-2024-07-11-00-25-58-687\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-07-11-00-25-58-687/source/sourcedir.tar.gz\",\"module_name\":\"estimator_entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"estimator_entry.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 estimator_entry.py\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,592 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:32:19,593 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mSuccessfully installed deepspeed-0.14.0 hjson-3.1.0 py-cpuinfo-9.0.0 pynvml-11.5.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.0 -> 24.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,652 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,652 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,712 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,761 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,810 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,822 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"sm-qwen2-multinode-2024-07-11-00-25-58-687\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-07-11-00-25-58-687/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"estimator_entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"estimator_entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=estimator_entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=estimator_entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-07-11-00-25-58-687/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"sm-qwen2-multinode-2024-07-11-00-25-58-687\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/sm-qwen2-multinode-2024-07-11-00-25-58-687/source/sourcedir.tar.gz\",\"module_name\":\"estimator_entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"estimator_entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 estimator_entry.py\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,824 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-07-11 00:32:19,824 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mObtaining file:///opt/ml/code/LLaMA-Factory\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/LLaMA-Factory\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[35mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting transformers>=4.41.2 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 6.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets>=2.16.0 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting accelerate>=0.30.1 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mCollecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.2.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.8.0)\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting tiktoken (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.20.3)\u001b[0m\n",
      "\u001b[35mCollecting uvicorn (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.7.2)\u001b[0m\n",
      "\u001b[35mCollecting fastapi (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[35mCollecting sse-starlette (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.8.4)\u001b[0m\n",
      "\u001b[35mCollecting fire (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading fire-0.6.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 15.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers>=4.41.2 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.16.0 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.30.1 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.7.2)\u001b[0m\n",
      "\u001b[34mCollecting fastapi (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (23.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.26.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (2.2.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.3.1 (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.14.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (16.1.0)\u001b[0m\n",
      "\u001b[35mCollecting pyarrow-hotfix (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0) (2024.5.0)\u001b[0m\n",
      "\u001b[34mCollecting sse-starlette (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.8.4)\u001b[0m\n",
      "\u001b[34mCollecting fire (from llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading fire-0.6.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (16.1.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0) (2024.5.0)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting altair<6.0,>=4.2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading ffmpy-0.3.2.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting gradio-client==1.0.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[35mCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.5)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting altair<6.0,>=4.2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.2.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==1.0.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.5)\u001b[0m\n",
      "\u001b[35mCollecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 9.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (10.3.0)\u001b[0m\n",
      "\u001b[35mCollecting pydub (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[35mCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.11.0)\u001b[0m\n",
      "\u001b[35mCollecting urllib3~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.52.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 5.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting pydub (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.11.0)\u001b[0m\n",
      "\u001b[34mCollecting urllib3~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.52.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (2.18.3)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (2.18.3)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\u001b[0m\n",
      "\u001b[35mCollecting h11>=0.8 (from uvicorn->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting termcolor (from fire->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[35mCollecting anyio (from sse-starlette->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.22.0)\u001b[0m\n",
      "\u001b[35mCollecting toolz (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[35mCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0) (3.7)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting termcolor (from fire->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting anyio (from sse-starlette->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.22.0)\u001b[0m\n",
      "\u001b[34mCollecting toolz (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0) (3.7)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.2.2)\u001b[0m\n",
      "\u001b[35mCollecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[35mCollecting sniffio (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.2.2)\u001b[0m\n",
      "\u001b[34mCollecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting sniffio (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.1)\u001b[0m\n",
      "\u001b[35mCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[35mCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[35mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.35.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.18.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\u001b[0m\n",
      "\u001b[34mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.1/314.1 kB 30.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 40.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 98.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.2/318.2 kB 39.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[35mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 33.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 104.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 31.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.111.0-py3-none-any.whl (91 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 78.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 71.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mDownloading altair-5.3.0-py3-none-any.whl (857 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 857.8/857.8 kB 63.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[35mDownloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 64.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 10.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 14.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 14.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 402.6/402.6 kB 48.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 23.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 57.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 101.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->llamafactory==0.8.3.dev0) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.1/314.1 kB 33.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 45.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 92.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.2/318.2 kB 31.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 31.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 100.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 33.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 10.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.111.0-py3-none-any.whl (91 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 16.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 64.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading altair-5.3.0-py3-none-any.whl (857 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 857.8/857.8 kB 61.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[34mDownloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 65.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 12.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 402.6/402.6 kB 32.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 71.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 14.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading anyio-4.4.0-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 15.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 91.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading typer-0.12.3-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.2/47.2 kB 9.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.4/103.4 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.6/53.6 kB 8.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.4/121.4 kB 19.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[35mDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 34.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 34.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.4/341.4 kB 43.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 21.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 94.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 75.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 33.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.1/56.1 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: llamafactory, fire, ffmpy\u001b[0m\n",
      "\u001b[35mBuilding editable for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding editable for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=18860 sha256=86a919c48d8e45847a02de863f0cfab0357ccea0dba6ebeba5e9d777281a184c\u001b[0m\n",
      "\u001b[35mStored in directory: /tmp/pip-ephem-wheel-cache-6l94jomi/wheels/04/23/a2/8fd431bb0a3883966b92993fae2f3d872c0c775856f573cfbb\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 52.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 101.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 60.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.4.0-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 86.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading typer-0.12.3-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.2/47.2 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.4/103.4 kB 12.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.6/53.6 kB 8.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.4/121.4 kB 17.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 24.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 35.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.4/341.4 kB 30.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 68.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 60.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.9/129.9 kB 17.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 27.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.1/56.1 kB 7.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: llamafactory, fire, ffmpy\u001b[0m\n",
      "\u001b[34mBuilding editable for llamafactory (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding editable for llamafactory (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=18860 sha256=2483b5548d1374afcf6a1863af268ac0cbe2685727a8cde64b0cf8af9056c630\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-kdgwcoi5/wheels/04/23/a2/8fd431bb0a3883966b92993fae2f3d872c0c775856f573cfbb\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=14967dd374f887f61e8a60d5f7b3f96a563a3f36edc9d53fdfbbe68d16b44294\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=64221daa90a0480285a47b1bce42aae82dd96a66e204564fb3d5f002eb0073e9\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\u001b[0m\n",
      "\u001b[35mSuccessfully built llamafactory fire ffmpy\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=14967dd374f887f61e8a60d5f7b3f96a563a3f36edc9d53fdfbbe68d16b44294\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=64221daa90a0480285a47b1bce42aae82dd96a66e204564fb3d5f002eb0073e9\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\u001b[0m\n",
      "\u001b[34mSuccessfully built llamafactory fire ffmpy\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sentencepiece, pydub, ffmpy, xxhash, websockets, uvloop, urllib3, ujson, toolz, tomlkit, termcolor, sniffio, shtab, semantic-version, safetensors, ruff, regex, python-multipart, python-dotenv, pyarrow-hotfix, orjson, multidict, importlib-resources, httptools, h11, frozenlist, docstring-parser, dnspython, async-timeout, aiofiles, yarl, uvicorn, httpcore, fire, email_validator, anyio, aiosignal, watchfiles, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi-cli, altair, accelerate, transformers, fastapi, datasets, trl, peft, gradio, llamafactory\u001b[0m\n",
      "\u001b[35mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[35mFound existing installation: urllib3 1.26.19\u001b[0m\n",
      "\u001b[35mUninstalling urllib3-1.26.19:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled urllib3-1.26.19\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, pydub, ffmpy, xxhash, websockets, uvloop, urllib3, ujson, toolz, tomlkit, termcolor, sniffio, shtab, semantic-version, safetensors, ruff, regex, python-multipart, python-dotenv, pyarrow-hotfix, orjson, multidict, importlib-resources, httptools, h11, frozenlist, docstring-parser, dnspython, async-timeout, aiofiles, yarl, uvicorn, httpcore, fire, email_validator, anyio, aiosignal, watchfiles, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi-cli, altair, accelerate, transformers, fastapi, datasets, trl, peft, gradio, llamafactory\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.19\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.19:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.19\u001b[0m\n",
      "\u001b[35mAttempting uninstall: typer\u001b[0m\n",
      "\u001b[35mFound existing installation: typer 0.9.4\u001b[0m\n",
      "\u001b[35mUninstalling typer-0.9.4:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled typer-0.9.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typer\u001b[0m\n",
      "\u001b[34mFound existing installation: typer 0.9.4\u001b[0m\n",
      "\u001b[34mUninstalling typer-0.9.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typer-0.9.4\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[35mweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed accelerate-0.32.1 aiofiles-23.2.1 aiohttp-3.9.5 aiosignal-1.3.1 altair-5.3.0 anyio-4.4.0 async-timeout-4.0.3 datasets-2.20.0 dnspython-2.6.1 docstring-parser-0.16 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 frozenlist-1.4.1 gradio-4.37.2 gradio-client-1.0.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.4 importlib-resources-6.4.0 llamafactory-0.8.3.dev0 multidict-6.0.5 orjson-3.10.6 peft-0.11.1 pyarrow-hotfix-0.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 ruff-0.5.1 safetensors-0.4.3 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.2 starlette-0.37.2 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 transformers-4.42.3 trl-0.9.6 typer-0.12.3 tyro-0.8.5 ujson-5.10.0 urllib3-2.2.2 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 24.0 -> 24.1.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35msh: 1: wandb: not found\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/.gitignore /tmp/initial-model-path/.huggingface/.gitignore\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/.gitattributes.lock /tmp/initial-model-path/.huggingface/download/.gitattributes.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/config.json.metadata /tmp/initial-model-path/.huggingface/download/config.json.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/.gitattributes.metadata /tmp/initial-model-path/.huggingface/download/.gitattributes.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/LICENSE.metadata /tmp/initial-model-path/.huggingface/download/LICENSE.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/generation_config.json.lock /tmp/initial-model-path/.huggingface/download/generation_config.json.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/LICENSE.lock /tmp/initial-model-path/.huggingface/download/LICENSE.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/README.md.metadata /tmp/initial-model-path/.huggingface/download/README.md.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.gitattributes /tmp/initial-model-path/.gitattributes\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/merges.txt.lock /tmp/initial-model-path/.huggingface/download/merges.txt.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/README.md.lock /tmp/initial-model-path/.huggingface/download/README.md.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/config.json.lock /tmp/initial-model-path/.huggingface/download/config.json.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/generation_config.json.metadata /tmp/initial-model-path/.huggingface/download/generation_config.json.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/model.safetensors.metadata /tmp/initial-model-path/.huggingface/download/model.safetensors.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/merges.txt.metadata /tmp/initial-model-path/.huggingface/download/merges.txt.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer.json.lock /tmp/initial-model-path/.huggingface/download/tokenizer.json.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer_config.json.lock /tmp/initial-model-path/.huggingface/download/tokenizer_config.json.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/model.safetensors.lock /tmp/initial-model-path/.huggingface/download/model.safetensors.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer_config.json.metadata /tmp/initial-model-path/.huggingface/download/tokenizer_config.json.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer.json.metadata /tmp/initial-model-path/.huggingface/download/tokenizer.json.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/README.md /tmp/initial-model-path/README.md\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/vocab.json.metadata /tmp/initial-model-path/.huggingface/download/vocab.json.metadata\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/LICENSE /tmp/initial-model-path/LICENSE\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/vocab.json.lock /tmp/initial-model-path/.huggingface/download/vocab.json.lock\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/config.json /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/generation_config.json /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.32.1 aiofiles-23.2.1 aiohttp-3.9.5 aiosignal-1.3.1 altair-5.3.0 anyio-4.4.0 async-timeout-4.0.3 datasets-2.20.0 dnspython-2.6.1 docstring-parser-0.16 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 frozenlist-1.4.1 gradio-4.37.2 gradio-client-1.0.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.4 importlib-resources-6.4.0 llamafactory-0.8.3.dev0 multidict-6.0.5 orjson-3.10.6 peft-0.11.1 pyarrow-hotfix-0.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 ruff-0.5.1 safetensors-0.4.3 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.2 starlette-0.37.2 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 transformers-4.42.3 trl-0.9.6 typer-0.12.3 tyro-0.8.5 ujson-5.10.0 urllib3-2.2.2 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.0 -> 24.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/tokenizer_config.json /tmp/initial-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/merges.txt /tmp/initial-model-path/merges.txt\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/vocab.json /tmp/initial-model-path/vocab.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/tokenizer.json /tmp/initial-model-path/tokenizer.json\u001b[0m\n",
      "\u001b[34msh: 1: wandb: not found\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.gitattributes /tmp/initial-model-path/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/.gitattributes.metadata /tmp/initial-model-path/.huggingface/download/.gitattributes.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/.gitattributes.lock /tmp/initial-model-path/.huggingface/download/.gitattributes.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/config.json.metadata /tmp/initial-model-path/.huggingface/download/config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/.gitignore /tmp/initial-model-path/.huggingface/.gitignore\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/LICENSE.lock /tmp/initial-model-path/.huggingface/download/LICENSE.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/LICENSE.metadata /tmp/initial-model-path/.huggingface/download/LICENSE.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/README.md.lock /tmp/initial-model-path/.huggingface/download/README.md.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/README.md.metadata /tmp/initial-model-path/.huggingface/download/README.md.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/generation_config.json.lock /tmp/initial-model-path/.huggingface/download/generation_config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/config.json.lock /tmp/initial-model-path/.huggingface/download/config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/model.safetensors.metadata /tmp/initial-model-path/.huggingface/download/model.safetensors.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/generation_config.json.metadata /tmp/initial-model-path/.huggingface/download/generation_config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/merges.txt.metadata /tmp/initial-model-path/.huggingface/download/merges.txt.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer.json.lock /tmp/initial-model-path/.huggingface/download/tokenizer.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/merges.txt.lock /tmp/initial-model-path/.huggingface/download/merges.txt.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer_config.json.metadata /tmp/initial-model-path/.huggingface/download/tokenizer_config.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/vocab.json.lock /tmp/initial-model-path/.huggingface/download/vocab.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/vocab.json.metadata /tmp/initial-model-path/.huggingface/download/vocab.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/model.safetensors.lock /tmp/initial-model-path/.huggingface/download/model.safetensors.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/README.md /tmp/initial-model-path/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer.json.metadata /tmp/initial-model-path/.huggingface/download/tokenizer.json.metadata\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/generation_config.json /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/LICENSE /tmp/initial-model-path/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/.huggingface/download/tokenizer_config.json.lock /tmp/initial-model-path/.huggingface/download/tokenizer_config.json.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/tokenizer_config.json /tmp/initial-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/merges.txt /tmp/initial-model-path/merges.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/tokenizer.json /tmp/initial-model-path/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/config.json /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/vocab.json /tmp/initial-model-path/vocab.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/model.safetensors /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-east-1-633205212955/qwen2-train-dataset/alpaca_zh_demo.json /tmp/data-path/alpaca_zh_demo.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/Qwen2-0.5B-Instruct/model.safetensors /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/qwen2-train-dataset/alpaca_zh_demo.json /tmp/data-path/alpaca_zh_demo.json\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:32:52,154] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-07-11 00:32:52,154] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:32:52,154] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-07-11 00:32:52,154] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:32:52,202] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[35m[2024-07-11 00:32:52,202] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:32:52,202] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m[2024-07-11 00:32:52,202] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:03,765] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:03,863] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:03,965] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:04,066] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:03,727] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:03,735] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:03,739] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:03,829] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:05,953] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file vocab.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file vocab.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file merges.txt\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file merges.txt\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,025 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:06,031] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:06,156] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:05,902] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:05,902] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:05,902] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:06,002] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:06,002] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2159] 2024-07-11 00:33:06,110 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:313] 2024-07-11 00:33:06,317 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:313] 2024-07-11 00:33:06,317 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35m[WARNING|logging.py:313] 2024-07-11 00:33:06,241 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m[WARNING|logging.py:313] 2024-07-11 00:33:06,241 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:33:06,244] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 1000 examples [00:00, 58936.08 examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 7028.13 examples/s]\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[35mConverting format of dataset (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 5395.80 examples/s]\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34mGenerating train split: 1000 examples [00:00, 60611.33 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 6190.47 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:06 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:07 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:07 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:07 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:07 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:07 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:07 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=4):  25%|██▌       | 250/1000 [00:00<00:01, 488.98 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=4):  50%|█████     | 500/1000 [00:00<00:00, 912.15 examples/s]\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1266.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):  25%|██▌       | 250/1000 [00:00<00:01, 503.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4):  75%|███████▌  | 750/1000 [00:00<00:00, 1220.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1163.50 examples/s]\u001b[0m\n",
      "\u001b[34minput_ids:\u001b[0m\n",
      "\u001b[34m[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 102450, 62926, 104136, 89012, 22382, 44177, 101047, 100369, 99891, 101911, 5122, 102150, 101911, 33108, 8903, 63109, 36587, 1773, 151645, 198, 151644, 77091, 198, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\u001b[0m\n",
      "\u001b[34minputs:\u001b[0m\n",
      "\u001b[34m<|im_start|>system\u001b[0m\n",
      "\u001b[34mYou are a helpful assistant.<|im_end|>\u001b[0m\n",
      "\u001b[34m<|im_start|>user\u001b[0m\n",
      "\u001b[34m识别并解释给定列表中的两个科学理论：细胞理论和日心说。<|im_end|>\u001b[0m\n",
      "\u001b[34m<|im_start|>assistant\u001b[0m\n",
      "\u001b[34m细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\u001b[0m\n",
      "\u001b[34m日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\u001b[0m\n",
      "\u001b[34mlabel_ids:\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\u001b[0m\n",
      "\u001b[34mlabels:\u001b[0m\n",
      "\u001b[34m细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\u001b[0m\n",
      "\u001b[34m日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:731] 2024-07-11 00:33:08,253 >> loading configuration file /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:731] 2024-07-11 00:33:08,253 >> loading configuration file /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:800] 2024-07-11 00:33:08,255 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/initial-model-path\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:800] 2024-07-11 00:33:08,255 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/initial-model-path\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3553] 2024-07-11 00:33:08,283 >> loading weights file /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3553] 2024-07-11 00:33:08,283 >> loading weights file /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3698] 2024-07-11 00:33:08,292 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3698] 2024-07-11 00:33:08,292 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[35mRunning tokenizer on dataset (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 1158.96 examples/s]\u001b[0m\n",
      "\u001b[35minput_ids:\u001b[0m\n",
      "\u001b[35m[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 102450, 62926, 104136, 89012, 22382, 44177, 101047, 100369, 99891, 101911, 5122, 102150, 101911, 33108, 8903, 63109, 36587, 1773, 151645, 198, 151644, 77091, 198, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\u001b[0m\n",
      "\u001b[35minputs:\u001b[0m\n",
      "\u001b[35m<|im_start|>system\u001b[0m\n",
      "\u001b[35mYou are a helpful assistant.<|im_end|>\u001b[0m\n",
      "\u001b[35m<|im_start|>user\u001b[0m\n",
      "\u001b[35m识别并解释给定列表中的两个科学理论：细胞理论和日心说。<|im_end|>\u001b[0m\n",
      "\u001b[35m<|im_start|>assistant\u001b[0m\n",
      "\u001b[35m细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\u001b[0m\n",
      "\u001b[35m日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\u001b[0m\n",
      "\u001b[35mlabel_ids:\u001b[0m\n",
      "\u001b[35m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151645]\u001b[0m\n",
      "\u001b[35mlabels:\u001b[0m\n",
      "\u001b[35m细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\u001b[0m\n",
      "\u001b[35m日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|im_end|>\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:731] 2024-07-11 00:33:08,253 >> loading configuration file /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:731] 2024-07-11 00:33:08,253 >> loading configuration file /tmp/initial-model-path/config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:800] 2024-07-11 00:33:08,255 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/initial-model-path\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:800] 2024-07-11 00:33:08,255 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/initial-model-path\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3553] 2024-07-11 00:33:08,283 >> loading weights file /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3553] 2024-07-11 00:33:08,283 >> loading weights file /tmp/initial-model-path/model.safetensors\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3698] 2024-07-11 00:33:08,292 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:3698] 2024-07-11 00:33:08,292 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:08,498 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:08,498 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:08,497 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:08,497 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:09,259] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 291, num_elems = 0.63B\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4364] 2024-07-11 00:33:10,353 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4372] 2024-07-11 00:33:10,353 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /tmp/initial-model-path.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4364] 2024-07-11 00:33:10,353 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4372] 2024-07-11 00:33:10,353 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /tmp/initial-model-path.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:953] 2024-07-11 00:33:10,356 >> loading configuration file /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:953] 2024-07-11 00:33:10,356 >> loading configuration file /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:10,356 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:10,356 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4364] 2024-07-11 00:33:10,353 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4372] 2024-07-11 00:33:10,353 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /tmp/initial-model-path.\u001b[0m\n",
      "\u001b[35mIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4364] 2024-07-11 00:33:10,353 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\u001b[0m\n",
      "\u001b[35m[INFO|modeling_utils.py:4372] 2024-07-11 00:33:10,353 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /tmp/initial-model-path.\u001b[0m\n",
      "\u001b[35mIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:953] 2024-07-11 00:33:10,356 >> loading configuration file /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:953] 2024-07-11 00:33:10,356 >> loading configuration file /tmp/initial-model-path/generation_config.json\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:10,356 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[INFO|configuration_utils.py:1000] 2024-07-11 00:33:10,356 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[35m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[35mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35m[WARNING|trainer.py:592] 2024-07-11 00:33:10,373 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35m[WARNING|trainer.py:592] 2024-07-11 00:33:10,373 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:642] 2024-07-11 00:33:10,373 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[35mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:642] 2024-07-11 00:33:10,373 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[35mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[35m[INFO|deepspeed.py:329] 2024-07-11 00:33:10,561 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\u001b[0m\n",
      "\u001b[35m[INFO|deepspeed.py:329] 2024-07-11 00:33:10,561 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\u001b[0m\n",
      "\u001b[34m07/11/2024 00:33:10 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[34m[WARNING|trainer.py:592] 2024-07-11 00:33:10,408 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[WARNING|trainer.py:592] 2024-07-11 00:33:10,408 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:642] 2024-07-11 00:33:10,408 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:642] 2024-07-11 00:33:10,408 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:329] 2024-07-11 00:33:10,595 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:329] 2024-07-11 00:33:10,595 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py310_cu121/cpu_adam...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu121/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35m[1/4] /opt/conda/bin/nvcc --generate-dependencies-with-compile --dependency-output custom_cuda_kernel.cuda.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc --generate-dependencies-with-compile --dependency-output custom_cuda_kernel.cuda.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[35m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[35m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[35m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.918355703353882 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.924159288406372 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.93408179283142 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.943135976791382 seconds\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 31.012414932250977 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 31.03879690170288 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 31.039507150650024 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 31.023181676864624 seconds\u001b[0m\n",
      "\u001b[35mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[35mConfig: alpha=0.000002, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000002, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,500] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,509] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,510] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,510] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,523] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,523] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,523] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,523] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,707] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,708] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 1.14 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,708] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.54 GB, percent = 13.1%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,709] [INFO] [stage3.py:130:__init__] Reduce bucket size 802816\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,709] [INFO] [stage3.py:131:__init__] Prefetch bucket size 722534\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,893] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,893] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:43,894] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.54 GB, percent = 13.1%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 71552 in 121 params\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,095] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,096] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,096] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.55 GB, percent = 13.1%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,282] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,283] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,283] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 24.55 GB, percent = 13.1%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,598] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,598] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,599] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.98 GB, percent = 13.9%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,784] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,785] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:44,785] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.98 GB, percent = 13.9%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,130] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,130] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,131] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.21 GB, percent = 14.0%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,318] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,318] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.45 GB, percent = 14.7%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,932] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,933] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.19 GB         CA 0.76 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.46 GB, percent = 14.7%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:45,933] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2128] 2024-07-11 00:33:46,416 >> ***** Running training *****\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2129] 2024-07-11 00:33:46,416 >>   Num examples = 1,000\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2128] 2024-07-11 00:33:46,416 >> ***** Running training *****\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2129] 2024-07-11 00:33:46,416 >>   Num examples = 1,000\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2130] 2024-07-11 00:33:46,416 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2131] 2024-07-11 00:33:46,416 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2130] 2024-07-11 00:33:46,416 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2131] 2024-07-11 00:33:46,416 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2134] 2024-07-11 00:33:46,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2135] 2024-07-11 00:33:46,416 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2136] 2024-07-11 00:33:46,417 >>   Total optimization steps = 5\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2134] 2024-07-11 00:33:46,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2135] 2024-07-11 00:33:46,416 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2136] 2024-07-11 00:33:46,417 >>   Total optimization steps = 5\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2137] 2024-07-11 00:33:46,417 >>   Number of trainable parameters = 494,032,768\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2137] 2024-07-11 00:33:46,417 >>   Number of trainable parameters = 494,032,768\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,601] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,602] [INFO] [utils.py:801:see_memory_usage] MA 0.19 GB         Max_MA 0.7 GB         CA 1.02 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,602] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.04 GB, percent = 15.0%\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-06, 2e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,603] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": true, \n",
      "    \"contiguous_memory_optimization\": true, \n",
      "    \"cpu_checkpointing\": true, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f401ca68a30>\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,604] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   graph_harvesting ............. False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,605] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   train_batch_size ............. 8\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=802816 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=722534 param_persistence_threshold=8960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:33:46,606] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true, \n",
      "        \"cpu_checkpointing\": true, \n",
      "        \"contiguous_memory_optimization\": true, \n",
      "        \"synchronize_checkpoint_boundary\": false, \n",
      "        \"profile\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 8.028160e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 7.225344e+05, \n",
      "        \"stage3_param_persistence_threshold\": 8.960000e+03, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2128] 2024-07-11 00:33:46,606 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2129] 2024-07-11 00:33:46,606 >>   Num examples = 1,000\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2130] 2024-07-11 00:33:46,606 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2131] 2024-07-11 00:33:46,606 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2134] 2024-07-11 00:33:46,606 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2128] 2024-07-11 00:33:46,606 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2129] 2024-07-11 00:33:46,606 >>   Num examples = 1,000\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2130] 2024-07-11 00:33:46,606 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2131] 2024-07-11 00:33:46,606 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2134] 2024-07-11 00:33:46,606 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2135] 2024-07-11 00:33:46,606 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2135] 2024-07-11 00:33:46,606 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2136] 2024-07-11 00:33:46,606 >>   Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2137] 2024-07-11 00:33:46,607 >>   Number of trainable parameters = 494,032,768\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2136] 2024-07-11 00:33:46,606 >>   Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2137] 2024-07-11 00:33:46,607 >>   Number of trainable parameters = 494,032,768\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 1/5 [00:02<00:11,  2.88s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:07<00:10,  3.65s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:08<00:04,  2.50s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:09<00:01,  1.95s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:10<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3478] 2024-07-11 00:33:58,444 >> Saving model checkpoint to /tmp/tuned-model-path/checkpoint-5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3478] 2024-07-11 00:33:58,444 >> Saving model checkpoint to /tmp/tuned-model-path/checkpoint-5\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:472] 2024-07-11 00:33:58,446 >> Configuration saved in /tmp/tuned-model-path/checkpoint-5/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:472] 2024-07-11 00:33:58,446 >> Configuration saved in /tmp/tuned-model-path/checkpoint-5/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:769] 2024-07-11 00:33:58,447 >> Configuration saved in /tmp/tuned-model-path/checkpoint-5/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:769] 2024-07-11 00:33:58,447 >> Configuration saved in /tmp/tuned-model-path/checkpoint-5/generation_config.json\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:34:00,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_4_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:34:00,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_4_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:34:00,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2690] 2024-07-11 00:33:59,878 >> Model weights saved in /tmp/tuned-model-path/checkpoint-5/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2690] 2024-07-11 00:33:59,878 >> Model weights saved in /tmp/tuned-model-path/checkpoint-5/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2574] 2024-07-11 00:33:59,879 >> tokenizer config file saved in /tmp/tuned-model-path/checkpoint-5/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2574] 2024-07-11 00:33:59,879 >> tokenizer config file saved in /tmp/tuned-model-path/checkpoint-5/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2583] 2024-07-11 00:33:59,879 >> Special tokens file saved in /tmp/tuned-model-path/checkpoint-5/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2583] 2024-07-11 00:33:59,879 >> Special tokens file saved in /tmp/tuned-model-path/checkpoint-5/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:00,089] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5 is about to be saved!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:00,097] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:00,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:00,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:00,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:34:01,150] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:34:01,150] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-07-11 00:34:01,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5 is ready now!\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2383] 2024-07-11 00:34:01,179 >> \u001b[0m\n",
      "\u001b[35mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[35m[INFO|trainer.py:2383] 2024-07-11 00:34:01,179 >> \u001b[0m\n",
      "\u001b[35mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:01,152] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:01,153] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-07-11 00:34:01,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5 is ready now!\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2383] 2024-07-11 00:34:01,179 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2383] 2024-07-11 00:34:01,179 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 14.5717, 'train_samples_per_second': 2.745, 'train_steps_per_second': 0.343, 'train_loss': 1.9626182556152343, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:14<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:14<00:00,  2.91s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3478] 2024-07-11 00:34:02,242 >> Saving model checkpoint to /tmp/tuned-model-path\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3478] 2024-07-11 00:34:02,242 >> Saving model checkpoint to /tmp/tuned-model-path\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:472] 2024-07-11 00:34:02,244 >> Configuration saved in /tmp/tuned-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:472] 2024-07-11 00:34:02,244 >> Configuration saved in /tmp/tuned-model-path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:769] 2024-07-11 00:34:02,245 >> Configuration saved in /tmp/tuned-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:769] 2024-07-11 00:34:02,245 >> Configuration saved in /tmp/tuned-model-path/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2690] 2024-07-11 00:34:03,593 >> Model weights saved in /tmp/tuned-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2690] 2024-07-11 00:34:03,593 >> Model weights saved in /tmp/tuned-model-path/model.safetensors\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2574] 2024-07-11 00:34:03,593 >> tokenizer config file saved in /tmp/tuned-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2574] 2024-07-11 00:34:03,593 >> tokenizer config file saved in /tmp/tuned-model-path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2583] 2024-07-11 00:34:03,593 >> Special tokens file saved in /tmp/tuned-model-path/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2583] 2024-07-11 00:34:03,593 >> Special tokens file saved in /tmp/tuned-model-path/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       0.04\n",
      "  total_flos               =        2GF\n",
      "  train_loss               =     1.9626\n",
      "  train_runtime            = 0:00:14.57\n",
      "  train_samples_per_second =      2.745\n",
      "  train_steps_per_second   =      0.343\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2024-07-11 00:34:03,767 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2024-07-11 00:34:03,767 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/added_tokens.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/latest s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/latest\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/added_tokens.json s3://sagemaker-us-east-1-633205212955/output-model/2406/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/README.md s3://sagemaker-us-east-1-633205212955/output-model/2406/README.md\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/training_args.bin s3://sagemaker-us-east-1-633205212955/output-model/2406/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/rng_state_0.pth s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/rng_state_0.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/trainer_state.json s3://sagemaker-us-east-1-633205212955/output-model/2406/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/tokenizer_config.json s3://sagemaker-us-east-1-633205212955/output-model/2406/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/zero_to_fp32.py s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/zero_to_fp32.py\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/tokenizer_config.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/train_results.json s3://sagemaker-us-east-1-633205212955/output-model/2406/train_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/scheduler.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/scheduler.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/special_tokens_map.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/all_results.json s3://sagemaker-us-east-1-633205212955/output-model/2406/all_results.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/trainer_log.jsonl s3://sagemaker-us-east-1-633205212955/output-model/2406/trainer_log.jsonl\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/trainer_state.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/trainer_state.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/training_args.bin s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/config.json s3://sagemaker-us-east-1-633205212955/output-model/2406/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_2_mp_rank_00_model_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/zero_pp_rank_2_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/generation_config.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/generation_config.json s3://sagemaker-us-east-1-633205212955/output-model/2406/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/rng_state_1.pth s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/rng_state_1.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/rng_state_3.pth s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/rng_state_3.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/special_tokens_map.json s3://sagemaker-us-east-1-633205212955/output-model/2406/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/rng_state_2.pth s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/rng_state_2.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/vocab.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/vocab.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_1_mp_rank_00_model_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/zero_pp_rank_1_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/merges.txt s3://sagemaker-us-east-1-633205212955/output-model/2406/merges.txt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/vocab.json s3://sagemaker-us-east-1-633205212955/output-model/2406/vocab.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/config.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/merges.txt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/merges.txt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/zero_pp_rank_3_mp_rank_00_model_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/zero_pp_rank_3_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/tokenizer.json s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/tokenizer.json s3://sagemaker-us-east-1-633205212955/output-model/2406/tokenizer.json\u001b[0m\n",
      "\u001b[35m2024-07-11 00:34:07,763 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:34:07,764 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-07-11 00:34:07,764 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/global_step5/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/global_step5/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/checkpoint-5/model.safetensors s3://sagemaker-us-east-1-633205212955/output-model/2406/checkpoint-5/model.safetensors\u001b[0m\n",
      "\u001b[34mcp /tmp/tuned-model-path/model.safetensors s3://sagemaker-us-east-1-633205212955/output-model/2406/model.safetensors\u001b[0m\n",
      "\u001b[34m2024-07-11 00:34:11,516 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-11 00:34:11,516 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-11 00:34:11,517 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker'\n",
    "# image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker'\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"    # 4 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.g5.48xlarge\"    # 8 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.p4d.24xlarge\"   # 8 * A100 (40G/GPU)\n",
    "# instance_type = \"ml.p5.48xlarge\"    # 8 * H100 (80G/GPU)\n",
    "\n",
    "instance_count = 2                  # Multi-node\n",
    "\n",
    "envs = {\n",
    "    # 'MODEL_ID_OR_S3_PATH': 's3://llm-artifacts-us-east-1/Qwen2-7B-Instruct/*',\n",
    "    \"DATA_S3_PATH\": f's3://{sagemaker_default_bucket}/qwen2-train-dataset/*',\n",
    "    'MODEL_ID_OR_S3_PATH': f's3://{sagemaker_default_bucket}/Qwen2-0.5B-Instruct/*',\n",
    "    'MODEL_SAVE_PATH_S3': f's3://{sagemaker_default_bucket}/output-model/2406/'\n",
    "}\n",
    "\n",
    "hypers = {\n",
    "}\n",
    "\n",
    "smp_estimator = Estimator(role=role,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name='sm-qwen2-multinode',\n",
    "    entry_point=\"estimator_entry.py\",\n",
    "    source_dir='submit_src/',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    environment=envs,\n",
    "    hyperparameters=hypers,\n",
    "    image_uri=image_uri,\n",
    "    input_mode=\"FastFile\",\n",
    "    # output_path=s3_output_bucket,\n",
    "    # checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    max_run=7200,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    enable_remote_debug=True,\n",
    "    disable_output_compression=True,\n",
    ")\n",
    "\n",
    "smp_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$sagemaker_default_bucket/output-model/2406/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
